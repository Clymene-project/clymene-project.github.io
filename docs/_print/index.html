<!doctype html><html lang=en class=no-js>
<head>
<meta charset=utf-8>
<meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no">
<meta name=generator content="Hugo 0.92.0">
<link rel=canonical type=text/html href=/docs/>
<link rel=alternate type=application/rss+xml href=/docs/index.xml>
<meta name=robots content="noindex, nofollow">
<link rel="shortcut icon" href=/favicons/favicon.ico>
<link rel=apple-touch-icon href=/favicons/apple-touch-icon-180x180.png sizes=180x180>
<link rel=icon type=image/png href=/favicons/favicon-16x16.png sizes=16x16>
<link rel=icon type=image/png href=/favicons/favicon-32x32.png sizes=32x32>
<link rel=icon type=image/png href=/favicons/android-36x36.png sizes=36x36>
<link rel=icon type=image/png href=/favicons/android-48x48.png sizes=48x48>
<link rel=icon type=image/png href=/favicons/android-72x72.png sizes=72x72>
<link rel=icon type=image/png href=/favicons/android-96x96.png sizes=96x96>
<link rel=icon type=image/png href=/favicons/android-144x144.png sizes=144x144>
<link rel=icon type=image/png href=/favicons/android-192x192.png sizes=192x192>
<title>Documentation | Clymene-project</title>
<meta name=description content="
    
          
The Clymene is a time series data collection platform for distributed systems inspired by Prometheus
and Jaeger. Time series data …">
<meta property="og:title" content="Documentation">
<meta property="og:description" content="This is Clymene project official docs">
<meta property="og:type" content="website">
<meta property="og:url" content="/docs/"><meta property="og:site_name" content="Clymene-project">
<meta itemprop=name content="Documentation">
<meta itemprop=description content="This is Clymene project official docs"><meta name=twitter:card content="summary">
<meta name=twitter:title content="Documentation">
<meta name=twitter:description content="This is Clymene project official docs">
<link rel=preload href=/scss/main.min.704093cd1a56f37fd00dc35c7125fced09d88d60c92d59965b30c8c0eeef1cd5.css as=style>
<link href=/scss/main.min.704093cd1a56f37fd00dc35c7125fced09d88d60c92d59965b30c8c0eeef1cd5.css rel=stylesheet integrity>
<script src=https://code.jquery.com/jquery-3.5.1.min.js integrity="sha256-9/aliU8dGd2tb6OSsuzixeV4y/faTqgFtohetphbbj0=" crossorigin=anonymous></script>
<script type=application/javascript>var doNotTrack=!1;doNotTrack||(window.ga=window.ga||function(){(ga.q=ga.q||[]).push(arguments)},ga.l=+new Date,ga('create','UA-00000000-0','auto'),ga('send','pageview'))</script>
<script async src=https://www.google-analytics.com/analytics.js></script>
</head>
<body class=td-section>
<header>
<nav class="js-navbar-scroll navbar navbar-expand navbar-dark flex-column flex-md-row td-navbar">
<a class=navbar-brand href=/>
<span class=navbar-logo></span><span class="text-uppercase font-weight-bold">Clymene-project</span>
</a>
<div class="td-navbar-nav-scroll ml-md-auto" id=main_navbar>
<ul class="navbar-nav mt-2 mt-lg-0">
<li class="nav-item mr-4 mb-2 mb-lg-0">
<a class="nav-link active" href=/docs/><span class=active>Documentation</span></a>
</li>
<li class="nav-item mr-4 mb-2 mb-lg-0">
<a class=nav-link href=/blog/><span>Releases</span></a>
</li>
</ul>
</div>
<div class="navbar-nav d-none d-lg-block"></div>
</nav>
</header>
<div class="container-fluid td-outer">
<div class=td-main>
<div class="row flex-xl-nowrap">
<main class="col-12 col-md-9 col-xl-8 pl-md-5" role=main>
<div class=td-content>
<div class="pageinfo pageinfo-primary d-print-none">
<p>
This is the multi-page printable view of this section.
<a href=# onclick="return print(),!1">Click here to print</a>.
</p><p>
<a href=/docs/>Return to the regular view of this page</a>.
</p>
</div>
<h1 class=title>Documentation</h1>
<ul>
<li>1: <a href=#pg-6e17e09fffc1050f46600282def85180>Overview</a></li>
<ul>
</ul>
<li>2: <a href=#pg-93aadc1aba179e6928539400a09b9e4e>Getting Started</a></li>
<ul>
<li>2.1: <a href=#pg-c211710cbee0593bb38d25f7975e1194>Clymene Agent</a></li>
<li>2.2: <a href=#pg-c5ce9be0274b23a88dee6db6cf3437ab>Clymene Gateway</a></li>
<li>2.3: <a href=#pg-637872d485c9581998ef23e19c928de0>Clymene Ingester</a></li>
</ul>
<li>3: <a href=#pg-fb4942d006e14ef10912cba7a3fabacd>Database Options</a></li>
<ul>
<li>3.1: <a href=#pg-637ffb17e80bb3ade4e7d9b697cfee37>Cortex Options</a></li>
<li>3.2: <a href=#pg-842fa5a3c1a3dc527d1c60e25f312969>ElasticSearch Options</a></li>
<li>3.3: <a href=#pg-92ee0a90a27f32e663d81f846f230895>InfluxDB Options</a></li>
<li>3.4: <a href=#pg-48a7486c70598ab850d05e9edd8674c7>Kafka Options</a></li>
<li>3.5: <a href=#pg-97cfc54549393296293902c0c1b65fdf>OpenTSDB Options</a></li>
<li>3.6: <a href=#pg-1db9969384a5a8315b7f84b832c15b52>Prometheus Options</a></li>
</ul>
<li>4: <a href=#pg-8dbd4e302cd86a4d0f71f71ce3b300de>Tutorials</a></li>
<ul>
<li>4.1: <a href=#pg-9f2233bbae28849bca70eebc1abaab70>Docker-compose Example</a></li>
<li>4.2: <a href=#pg-2193a6415e395bfca415a334c159ad21>k8s(Kubernetes) Example</a></li>
</ul>
<li>5: <a href=#pg-8efbf7499fe9c7bde8d2c1bb7ff6ee3c></a></li>
<ul>
</ul>
</ul>
<div class=content>
<p><img src=https://user-images.githubusercontent.com/25188468/149839035-f357caea-d335-4b24-88e7-e957f79dc6be.png alt=clymene_logo_v1_side></p>
<p><a href=https://bestpractices.coreinfrastructure.org/projects/5491><img src=https://bestpractices.coreinfrastructure.org/projects/5491/badge alt="CII Best Practices"></a> <img src=https://github.com/clymene-project/clymene/workflows/CodeQL/badge.svg alt=CodeQL> <a href=https://opensource.org/licenses/Apache-2.0><img src=https://img.shields.io/badge/License-Apache%202.0-blue.svg alt=License></a> <img src=https://img.shields.io/github/v/release/clymene-project/clymene alt="GitHub release (latest by date)"> <a href=https://pkg.go.dev/github.com/Clymene-project/Clymene><img src=https://pkg.go.dev/badge/github.com/Clymene-project/Clymene.svg alt="Go Reference"></a><br>
<img src="https://img.shields.io/badge/go-%2300ADD8.svg?style=for-the-badge&logo=go&logoColor=white" alt=Go> <img src="https://img.shields.io/badge/docker-%230db7ed.svg?style=for-the-badge&logo=docker&logoColor=white" alt=Docker> <img src="https://img.shields.io/badge/kubernetes-%23326ce5.svg?style=for-the-badge&logo=kubernetes&logoColor=white" alt=Kubernetes> <img src="https://img.shields.io/badge/-ElasticSearch-005571?style=for-the-badge&logo=elasticsearch" alt=ElasticSearch> <img src="https://img.shields.io/badge/influxdb-%2322ADF6.svg?&style=for-the-badge&logo=influxdb&logoColor=white"> <img src="https://img.shields.io/badge/prometheus-%23E6522C.svg?&style=for-the-badge&logo=prometheus&logoColor=white"> <img src="https://img.shields.io/badge/OpenTSDB-green?style=for-the-badge"> <img src="https://img.shields.io/badge/cortex-blue?style=for-the-badge"></p>
<p>The Clymene is a time series data collection platform for distributed systems inspired by <a href=https://prometheus.io>Prometheus</a>
and <a href=https://www.jaegertracing.io>Jaeger</a>. Time series data from various environments can be collected and stored in different types of databases. It can be configured in a variety of architectures.</p>
<ul>
<li><strong>Time-series data collection platform:</strong> Clymene uses fewer resources than <a href=https://prometheus.io>Prometheus</a> to collect metrics. In addition, various architectures can be constructed using ingester and gateway components.</li>
<li><strong>heterogeneous database support:</strong> Time series data created with Prometheus exporter can be stored in various databases such as ElasticSearch, Prometheus, Cortex, kdb++, OpenTSDB, InfluxDB and TDengine</li>
</ul>
</div>
</div>
<div class=td-content style=page-break-before:always>
<h1 id=pg-6e17e09fffc1050f46600282def85180>1 - Overview</h1>
<div class=lead>Overview introduces why Clymene should be used, architecture, and components.</div>
<img align=right width=400 height=400 src=https://user-images.githubusercontent.com/25188468/148681479-3ddf237c-6e5d-49a1-a517-8b3bfa92f54e.png alt=clymene_logo>
Clymene, developed as an architecture that collects time series data from various environments, can provide observability by collecting time series data from multiple complex and various environments and systems in one place.
<p>Clymene agent does not use WAL(Write Ahead Log), it uses fewer resources than <a href=https://prometheus.io>Prometheus</a> when collecting metrics.</p>
<p>In addition, various architectures can be constructed using ingester and gateway components.</p>
<h2 id=clymene-project>Clymene-project?</h2>
<p>Clymene is an open source developed based on Prometheus’ time series data collection method and the architecture of the Jaegertracing project, an open source of service tracing in a distributed environment.
Prometheus’ Service Discovery features make it easy to find endpoints that can collect time series data from multiple environments, and can be configured with a powerful high-availability (HA) architecture using Jaeger’s backend structure.</p>
<h2 id=why-do-i-have-to-use-clymene>Why do I have to use Clymene?</h2>
<p>Help your user know if your project will help them. Useful information can include:</p>
<ul>
<li><strong>Time-series data collection platform:</strong> Clymene uses fewer resources than <a href=https://prometheus.io>Prometheus</a> to collect metrics. In addition, various architectures can be constructed using ingester and gateway components.</li>
<li><strong>Collect metrics from various environments:</strong> Multiple Kubernetes clusters and non-kubernetes environments, not one Kubernetes cluster, also when you want to collect and monitor metrics in one place.</li>
<li><strong>heterogeneous database support:</strong> Time series data created with Prometheus exporter can be stored in various databases such as ElasticSearch, Prometheus, Cortex, kdb++, OpenTSDB, InfluxDB and TDengine</li>
<li><strong>Long term storage:</strong> Clymene supports a variety of databases, allowing users to handle time series data in familiar databases.</li>
</ul>
<h2 id=components>Components</h2>
<h4 id=clymene-agent>Clymene Agent</h4>
<p>The Clymene-agent is service that collects time series data(does not use disks)</p>
<ol>
<li>Service Discovery
<ul>
<li><a href=https://docs.sysdig.com/en/docs/sysdig-monitor/integrations-for-sysdig-monitor/collect-prometheus-metrics/enable-prometheus-native-service-discovery/>Prometheus&rsquo;s Service Discovery</a>
feature finds Metric collection endpoints.</li>
</ul>
</li>
<li>scrape time series data</li>
<li>Time-series data transfer to gateway(gRPC) (Optional)</li>
<li>Time-series data transfer to kafka (Optional)</li>
<li>Time-series data insert to Database(<a href=https://github.com/Clymene-project/Clymene/blob/main/docs/clymene-agent/README.md#Option-description-by-storage-type>supported DB</a>) (Optional)</li>
</ol>
<h4 id=clymene-ingester>Clymene Ingester</h4>
<p>The Clymene ingester is an optional service responsible for insert time series data loaded on kafka into the database.</p>
<ol>
<li>Kafka message consume</li>
<li>Time-series data insert to Database(<a href=https://github.com/Clymene-project/Clymene/blob/main/docs/clymene-ingester/README.md#Option-description-by-storage-type>supported DB</a>) (Optional)</li>
</ol>
<h4 id=clymene-gateway>Clymene Gateway</h4>
<p>The Clymene Gateway is an optional service that can receive metric data from the another component through gRPC
communication.</p>
<ol>
<li>gRPC Service</li>
<li>Time-series data insert to Database(<a href=https://github.com/Clymene-project/Clymene/blob/main/docs/clymene-gateway/README.md#Option-description-by-storage-type>supported DB</a>) (Optional)</li>
</ol>
<h2 id=roadmap>Roadmap</h2>
<ul>
<li>Clymene&rsquo;s short-term goal is to support heterogeneous databases in Grafana as a query.</li>
<li><a href=https://github.com/Clymene-project/clymene-analyzer>AI/ML platform for clymene</a></li>
<li><a href=https://github.com/Clymene-project/Clymene/discussions>Please give me an idea!</a></li>
</ul>
</div>
<div class=td-content style=page-break-before:always>
<h1 id=pg-93aadc1aba179e6928539400a09b9e4e>2 - Getting Started</h1>
<div class=lead>Introducing how to use Clymene&rsquo;s component.</div>
<p>The Clymene project offers both Docker images and binary forms. Accordingly, metrics can be collected in various environments such as the kubernetes environment and Openstack.
Getting Started introduces how to use each component.</p>
<h2 id=download>Download</h2>
<p>First, download Clymene according to the OS version
<a href=https://github.com/Clymene-project/Clymene/releases>Download link</a></p>
<h2 id=try-it-out>Try it out!</h2>
<p>Look at how to use each Clymene component and follow it.</p>
</div>
<div class=td-content style=page-break-before:always>
<h1 id=pg-c211710cbee0593bb38d25f7975e1194>2.1 - Clymene Agent</h1>
<div class=lead>The Clymene-agent is service that collects time series data</div>
<h1 id=clymene-agent-getting-start>Clymene-agent Getting Start</h1>
<p>The Clymene-agent is service that collects time series data(does not use disks)</p>
<h3 id=how-to-create-a-scrape-target-setting-yaml>How to create a scrape target setting yaml</h3>
<ol>
<li>Config file Option</li>
</ol>
<div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash>clymene-agent --config.file<span style=color:#ce5c00;font-weight:700>=</span>/etc/clymene/clymene.yml
</code></pre></div><ol start=2>
<li>How to write yaml - <a href=https://prometheus.io/docs/prometheus/latest/configuration/configuration/>See Prometheus Configuration for more information</a></li>
</ol>
<div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=color:#204a87;font-weight:700>global</span><span style=color:#000;font-weight:700>:</span><span style=color:#f8f8f8;text-decoration:underline>
</span><span style=color:#f8f8f8;text-decoration:underline>   </span><span style=color:#204a87;font-weight:700>scrape_interval</span><span style=color:#000;font-weight:700>:</span><span style=color:#f8f8f8;text-decoration:underline> </span><span style=color:#000>15s</span><span style=color:#f8f8f8;text-decoration:underline> </span><span style=color:#8f5902;font-style:italic># Set the scrape interval to every 15 seconds. Default is every 1 minute.  </span><span style=color:#f8f8f8;text-decoration:underline>
</span><span style=color:#f8f8f8;text-decoration:underline>   </span><span style=color:#204a87;font-weight:700>evaluation_interval</span><span style=color:#000;font-weight:700>:</span><span style=color:#f8f8f8;text-decoration:underline> </span><span style=color:#000>15s</span><span style=color:#f8f8f8;text-decoration:underline> </span><span style=color:#8f5902;font-style:italic># Evaluate rules every 15 seconds. The default is every 1 minute.</span><span style=color:#f8f8f8;text-decoration:underline>
</span><span style=color:#f8f8f8;text-decoration:underline>
</span><span style=color:#f8f8f8;text-decoration:underline></span><span style=color:#204a87;font-weight:700>scrape_configs</span><span style=color:#000;font-weight:700>:</span><span style=color:#f8f8f8;text-decoration:underline>
</span><span style=color:#f8f8f8;text-decoration:underline></span>- <span style=color:#204a87;font-weight:700>job_name</span><span style=color:#000;font-weight:700>:</span><span style=color:#f8f8f8;text-decoration:underline> </span><span style=color:#4e9a06>&#39;localhost&#39;</span><span style=color:#f8f8f8;text-decoration:underline>  
</span><span style=color:#f8f8f8;text-decoration:underline>  </span><span style=color:#204a87;font-weight:700>static_configs</span><span style=color:#000;font-weight:700>:</span><span style=color:#f8f8f8;text-decoration:underline>  
</span><span style=color:#f8f8f8;text-decoration:underline>    </span>- <span style=color:#204a87;font-weight:700>targets</span><span style=color:#000;font-weight:700>:</span><span style=color:#f8f8f8;text-decoration:underline> </span><span style=color:#000;font-weight:700>[</span><span style=color:#f8f8f8;text-decoration:underline> </span><span style=color:#4e9a06>&#39;localhost:9100&#39;</span><span style=color:#f8f8f8;text-decoration:underline> </span><span style=color:#000;font-weight:700>]</span><span style=color:#f8f8f8;text-decoration:underline>   
</span><span style=color:#f8f8f8;text-decoration:underline>
</span><span style=color:#f8f8f8;text-decoration:underline></span>- <span style=color:#204a87;font-weight:700>job_name</span><span style=color:#000;font-weight:700>:</span><span style=color:#f8f8f8;text-decoration:underline> </span><span style=color:#4e9a06>&#39;kubernetes-kubelet&#39;</span><span style=color:#f8f8f8;text-decoration:underline>  
</span><span style=color:#f8f8f8;text-decoration:underline>  </span><span style=color:#204a87;font-weight:700>scheme</span><span style=color:#000;font-weight:700>:</span><span style=color:#f8f8f8;text-decoration:underline> </span><span style=color:#000>https  </span><span style=color:#f8f8f8;text-decoration:underline>
</span><span style=color:#f8f8f8;text-decoration:underline>  </span><span style=color:#204a87;font-weight:700>tls_config</span><span style=color:#000;font-weight:700>:</span><span style=color:#f8f8f8;text-decoration:underline>  
</span><span style=color:#f8f8f8;text-decoration:underline>  </span><span style=color:#204a87;font-weight:700>ca_file</span><span style=color:#000;font-weight:700>:</span><span style=color:#f8f8f8;text-decoration:underline> </span><span style=color:#000>/var/run/secrets/kubernetes.io/serviceaccount/ca.crt  </span><span style=color:#f8f8f8;text-decoration:underline>
</span><span style=color:#f8f8f8;text-decoration:underline>  </span><span style=color:#204a87;font-weight:700>bearer_token_file</span><span style=color:#000;font-weight:700>:</span><span style=color:#f8f8f8;text-decoration:underline> </span><span style=color:#000>/var/run/secrets/kubernetes.io/serviceaccount/token  </span><span style=color:#f8f8f8;text-decoration:underline>
</span><span style=color:#f8f8f8;text-decoration:underline>  </span><span style=color:#204a87;font-weight:700>kubernetes_sd_configs</span><span style=color:#000;font-weight:700>:</span><span style=color:#f8f8f8;text-decoration:underline>  
</span><span style=color:#f8f8f8;text-decoration:underline>    </span>- <span style=color:#204a87;font-weight:700>role</span><span style=color:#000;font-weight:700>:</span><span style=color:#f8f8f8;text-decoration:underline> </span><span style=color:#000>node  </span><span style=color:#f8f8f8;text-decoration:underline>
</span><span style=color:#f8f8f8;text-decoration:underline>      </span><span style=color:#204a87;font-weight:700>relabel_configs</span><span style=color:#000;font-weight:700>:</span><span style=color:#f8f8f8;text-decoration:underline>  
</span><span style=color:#f8f8f8;text-decoration:underline>    </span>- <span style=color:#204a87;font-weight:700>action</span><span style=color:#000;font-weight:700>:</span><span style=color:#f8f8f8;text-decoration:underline> </span><span style=color:#000>labelmap  </span><span style=color:#f8f8f8;text-decoration:underline>
</span><span style=color:#f8f8f8;text-decoration:underline>      </span><span style=color:#204a87;font-weight:700>regex</span><span style=color:#000;font-weight:700>:</span><span style=color:#f8f8f8;text-decoration:underline> </span><span style=color:#000>__meta_kubernetes_node_label_(.+)  </span><span style=color:#f8f8f8;text-decoration:underline>
</span><span style=color:#f8f8f8;text-decoration:underline>    </span>- <span style=color:#204a87;font-weight:700>target_label</span><span style=color:#000;font-weight:700>:</span><span style=color:#f8f8f8;text-decoration:underline> </span><span style=color:#000>__address__  </span><span style=color:#f8f8f8;text-decoration:underline>
</span><span style=color:#f8f8f8;text-decoration:underline>      </span><span style=color:#204a87;font-weight:700>replacement</span><span style=color:#000;font-weight:700>:</span><span style=color:#f8f8f8;text-decoration:underline> </span><span style=color:#000>kubernetes.default.svc:443  </span><span style=color:#f8f8f8;text-decoration:underline>
</span><span style=color:#f8f8f8;text-decoration:underline>    </span>- <span style=color:#204a87;font-weight:700>source_labels</span><span style=color:#000;font-weight:700>:</span><span style=color:#f8f8f8;text-decoration:underline> </span><span style=color:#000;font-weight:700>[</span><span style=color:#000>__meta_kubernetes_node_name]  </span><span style=color:#f8f8f8;text-decoration:underline>
</span><span style=color:#f8f8f8;text-decoration:underline>      </span><span style=color:#204a87;font-weight:700>regex</span><span style=color:#000;font-weight:700>:</span><span style=color:#f8f8f8;text-decoration:underline> </span><span style=color:#000>(.+)  </span><span style=color:#f8f8f8;text-decoration:underline>
</span><span style=color:#f8f8f8;text-decoration:underline>      </span><span style=color:#204a87;font-weight:700>target_label</span><span style=color:#000;font-weight:700>:</span><span style=color:#f8f8f8;text-decoration:underline> </span><span style=color:#000>__metrics_path__  </span><span style=color:#f8f8f8;text-decoration:underline>
</span><span style=color:#f8f8f8;text-decoration:underline>      </span><span style=color:#204a87;font-weight:700>replacement</span><span style=color:#000;font-weight:700>:</span><span style=color:#f8f8f8;text-decoration:underline> </span><span style=color:#000>/api/v1/nodes/${1}/proxy/metrics</span><span style=color:#f8f8f8;text-decoration:underline>
</span></code></pre></div><h3 id=how-to-set-up-the-storage-type>How to set up the Storage Type</h3>
<h5 id=1-setting-environmental-variables>1. Setting environmental variables</h5>
<p>ElasticSearch</p>
<pre tabindex=0><code>TS_STORAGE_TYPE=elasticsearch
</code></pre><p>Kafka</p>
<pre tabindex=0><code>TS_STORAGE_TYPE=kafka
</code></pre><p>prometheus</p>
<pre tabindex=0><code>TS_STORAGE_TYPE=prometheus
</code></pre><p>cortex</p>
<pre tabindex=0><code>TS_STORAGE_TYPE=cortex
</code></pre><p>gateway</p>
<pre tabindex=0><code>TS_STORAGE_TYPE=gateway
</code></pre><p>opentsdb</p>
<pre tabindex=0><code>TS_STORAGE_TYPE=opentsdb
</code></pre><p>influxdb</p>
<pre tabindex=0><code>TS_STORAGE_TYPE=influxdb
</code></pre><p>Several</p>
<pre tabindex=0><code>TS_STORAGE_TYPE=elasticsearch,prometheus  # composite write - Write in multiple databases at the same time.
</code></pre><h5 id=2-option-description-by-storage-type>2. Option description by storage type</h5>
<ul>
<li><a href=http://clymene-project.github.io/docs/database-options/kafka>Kafka option</a></li>
<li><a href=http://clymene-project.github.io/docs/database-options/elasticsearch>ElasticSearch option</a></li>
<li><a href=http://clymene-project.github.io/docs/database-options/prometheus>Prometheus option</a></li>
<li><a href=http://clymene-project.github.io/docs/database-options/cortex>cortex option</a></li>
<li><a href=http://clymene-project.github.io/docs/database-options/gateway>gateway option</a></li>
<li><a href=http://clymene-project.github.io/docs/database-options/opentsdb>Opentsdb option</a></li>
<li><a href=http://clymene-project.github.io/docs/database-options/influxdb>influxdb option</a></li>
</ul>
<h3 id=use-only-agent-architecture>Use only agent Architecture</h3>
<img src=https://user-images.githubusercontent.com/25188468/148680397-aa9ca8a8-810c-4141-aefb-7e5d8ed87d13.png width=70% height=70% alt=architecture_v1.3.0>
</div>
<div class=td-content style=page-break-before:always>
<h1 id=pg-c5ce9be0274b23a88dee6db6cf3437ab>2.2 - Clymene Gateway</h1>
<div class=lead>The Clymene Gateway is an optional service that can receive metric data from the agent through gRPC communication</div>
<h1 id=clymene-gateway-getting-start>Clymene Gateway Getting Start</h1>
<p>The Clymene Gateway is an optional service that can receive metric data from the agent through gRPC communication.</p>
<ol>
<li>gRPC Service</li>
<li>Time-series data insert to Database(ElasticSearch, Prometheus, ETC) (Optional)</li>
</ol>
<h3 id=how-to-setting-grpc-server>How to setting gRPC server</h3>
<pre tabindex=0><code>--gateway.grpc-server.host-port string      The host:port (e.g. 127.0.0.1:15610 or :15610) of the gateway's GRPC server (default &quot;:15610&quot;)
--gateway.grpc.tls.cert string              Path to a TLS Certificate file, used to identify this server to clients
--gateway.grpc.tls.client-ca string         Path to a TLS CA (Certification Authority) file used to verify certificates presented by clients (if unset, all clients are permitted)
--gateway.grpc.tls.enabled                  Enable TLS on the server
--gateway.grpc.tls.key string               Path to a TLS Private Key file, used to identify this server to clients
</code></pre><h3 id=how-to-set-up-the-storage-type>How to set up the Storage Type</h3>
<h5 id=1-setting-environmental-variables>1. Setting environmental variables</h5>
<p>ElasticSearch</p>
<pre tabindex=0><code>TS_STORAGE_TYPE=elasticsearch
</code></pre><p>Kafka</p>
<pre tabindex=0><code>TS_STORAGE_TYPE=kafka
</code></pre><p>prometheus</p>
<pre tabindex=0><code>TS_STORAGE_TYPE=prometheus
</code></pre><p>cortex</p>
<pre tabindex=0><code>TS_STORAGE_TYPE=cortex
</code></pre><p>opentsdb</p>
<pre tabindex=0><code>TS_STORAGE_TYPE=opentsdb
</code></pre><p>influxdb</p>
<pre tabindex=0><code>TS_STORAGE_TYPE=influxdb
</code></pre><p>Several</p>
<pre tabindex=0><code>TS_STORAGE_TYPE=elasticsearch,prometheus  # composite write - Write in multiple databases at the same time.
</code></pre><h5 id=2-option-description-by-storage-type>2. Option description by storage type</h5>
<ul>
<li><a href=http://clymene-project.github.io/docs/database-options/kafka>Kafka option</a></li>
<li><a href=http://clymene-project.github.io/docs/database-options/elasticsearch>ElasticSearch option</a></li>
<li><a href=http://clymene-project.github.io/docs/database-options/prometheus>Prometheus option</a></li>
<li><a href=http://clymene-project.github.io/docs/database-options/cortex>cortex option</a></li>
<li><a href=http://clymene-project.github.io/docs/database-options/gateway>gateway option</a></li>
<li><a href=http://clymene-project.github.io/docs/database-options/opentsdb>Opentsdb option</a></li>
<li><a href=http://clymene-project.github.io/docs/database-options/influxdb>influxdb option</a></li>
</ul>
<h3 id=use-gateway-architecture>Use gateway Architecture</h3>
<img src=https://user-images.githubusercontent.com/25188468/148680473-290f098e-fab6-4e87-a958-cdd96ff12a15.png width=70% height=70% alt=architecture_v1.3.0_gateway>
</div>
<div class=td-content style=page-break-before:always>
<h1 id=pg-637872d485c9581998ef23e19c928de0>2.3 - Clymene Ingester</h1>
<div class=lead>The Clymene ingester is an optional service responsible for insert time series data loaded on kafka into the database</div>
<h1 id=clymene-ingester-getting-start>Clymene Ingester Getting Start</h1>
<p>The Clymene ingester is an optional service responsible for insert time series data loaded on kafka into the database</p>
<ol>
<li>Kafka message consume</li>
<li>Time-series data insert to Database(ElasticSearch, Prometheus, ETC) (Optional)</li>
</ol>
<h3 id=how-to-setting-kafka-consumer>How to setting kafka consumer</h3>
<pre tabindex=0><code>--kafka.consumer.authentication string          Authentication type used to authenticate with kafka cluster. e.g. none, kerberos, tls, plaintext (default &quot;none&quot;)
--kafka.consumer.brokers string                 The comma-separated list of kafka brokers. i.e. '127.0.0.1:9092,0.0.0:1234' (default &quot;127.0.0.1:9092&quot;)
--kafka.consumer.client-id string               The Consumer Client ID that clymene-ingester will use (default &quot;clymene&quot;)
--kafka.consumer.encoding string                The encoding of metrics (&quot;json&quot;, &quot;protobuf&quot;) consumed from kafka (default &quot;protobuf&quot;)
--kafka.consumer.group-id string                The Consumer Group that clymene-ingester will be consuming on behalf of (default &quot;clymene&quot;)
--kafka.consumer.kerberos.config-file string    Path to Kerberos configuration. i.e /etc/krb5.conf (default &quot;/etc/krb5.conf&quot;)
--kafka.consumer.kerberos.keytab-file string    Path to keytab file. i.e /etc/security/kafka.keytab (default &quot;/etc/security/kafka.keytab&quot;)
--kafka.consumer.kerberos.password string       The Kerberos password used for authenticate with KDC
--kafka.consumer.kerberos.realm string          Kerberos realm
--kafka.consumer.kerberos.service-name string   Kerberos service name (default &quot;kafka&quot;)
--kafka.consumer.kerberos.use-keytab            Use of keytab instead of password, if this is true, keytab file will be used instead of password
--kafka.consumer.kerberos.username string       The Kerberos username used for authenticate with KDC
--kafka.consumer.plaintext.mechanism string     The plaintext Mechanism for SASL/PLAIN authentication, e.g. 'SCRAM-SHA-256' or 'SCRAM-SHA-512' or 'PLAIN' (default &quot;PLAIN&quot;)
--kafka.consumer.plaintext.password string      The plaintext Password for SASL/PLAIN authentication
--kafka.consumer.plaintext.username string      The plaintext Username for SASL/PLAIN authentication
--kafka.consumer.protocol-version string        Kafka protocol version - must be supported by kafka server
--kafka.consumer.tls.ca string                  Path to a TLS CA (Certification Authority) file used to verify the remote server(s) (by default will use the system truststore)
--kafka.consumer.tls.cert string                Path to a TLS Certificate file, used to identify this process to the remote server(s)
--kafka.consumer.tls.enabled                    Enable TLS when talking to the remote server(s)
--kafka.consumer.tls.key string                 Path to a TLS Private Key file, used to identify this process to the remote server(s)
--kafka.consumer.tls.server-name string         Override the TLS server name we expect in the certificate of the remote server(s)
--kafka.consumer.tls.skip-host-verify           (insecure) Skip server's certificate chain and host name verification
--kafka.consumer.topic string                   The name of the kafka topic to consume from (default &quot;clymene&quot;)
</code></pre><h3 id=how-to-set-up-the-storage-type>How to set up the Storage Type</h3>
<h5 id=1-setting-environmental-variables>1. Setting environmental variables</h5>
<p>ElasticSearch</p>
<pre tabindex=0><code>TS_STORAGE_TYPE=elasticsearch
</code></pre><p>Kafka</p>
<pre tabindex=0><code>TS_STORAGE_TYPE=kafka
</code></pre><p>prometheus</p>
<pre tabindex=0><code>TS_STORAGE_TYPE=prometheus
</code></pre><p>cortex</p>
<pre tabindex=0><code>TS_STORAGE_TYPE=cortex
</code></pre><p>gateway</p>
<pre tabindex=0><code>TS_STORAGE_TYPE=gateway
</code></pre><p>opentsdb</p>
<pre tabindex=0><code>TS_STORAGE_TYPE=opentsdb
</code></pre><p>influxdb</p>
<pre tabindex=0><code>TS_STORAGE_TYPE=influxdb
</code></pre><p>Several</p>
<pre tabindex=0><code>TS_STORAGE_TYPE=elasticsearch,prometheus  # composite write - Write in multiple databases at the same time.
</code></pre><h5 id=2-option-description-by-storage-type>2. Option description by storage type</h5>
<ul>
<li><a href=http://clymene-project.github.io/docs/database-options/kafka>Kafka option</a></li>
<li><a href=http://clymene-project.github.io/docs/database-options/elasticsearch>ElasticSearch option</a></li>
<li><a href=http://clymene-project.github.io/docs/database-options/prometheus>Prometheus option</a></li>
<li><a href=http://clymene-project.github.io/docs/database-options/cortex>cortex option</a></li>
<li><a href=http://clymene-project.github.io/docs/database-options/gateway>gateway option</a></li>
<li><a href=http://clymene-project.github.io/docs/database-options/opentsdb>Opentsdb option</a></li>
<li><a href=http://clymene-project.github.io/docs/database-options/influxdb>influxdb option</a></li>
</ul>
<h3 id=including-kafka-and-ingester-architecture>Including kafka and ingester Architecture</h3>
<img src=https://user-images.githubusercontent.com/25188468/148680487-de5b083d-024e-4aaa-aeac-d362bbcec8de.png width=70% height=70% alt=architecture_v1.3.0_ingester>
</div>
<div class=td-content style=page-break-before:always>
<h1 id=pg-fb4942d006e14ef10912cba7a3fabacd>3 - Database Options</h1>
<div class=lead>Introduce options for the database available.</div>
<p>Describe the database options available in Clymene</p>
</div>
<div class=td-content>
<h1 id=pg-637ffb17e80bb3ade4e7d9b697cfee37>3.1 - Cortex Options</h1>
<div class=lead><a href=https://cortexmetrics.io/>https://cortexmetrics.io/</a></div>
<h3 id=cortex-options>Cortex Options</h3>
<pre tabindex=0><code>--cortex.distributor.kafka.encoding string   Encoding of metric (&quot;json&quot; or &quot;protobuf&quot;) sent to kafka. (default &quot;protobuf&quot;)
--cortex.distributor.max.err.msg.len int     Maximum length of error message (default 256)
--cortex.distributor.timeout duration        Time out when doing remote write(sec, default 10 sec) (default 10s)
--cortex.distributor.url string              the cortex distributor remote write receiver endpoint(/api/v1/push) (default &quot;http://localhost/api/v1/push&quot;)
--cortex.distributor.user.agent string       User-Agent in request header (default &quot;Prometheus/&quot;)
</code></pre>
</div>
<div class=td-content style=page-break-before:always>
<h1 id=pg-842fa5a3c1a3dc527d1c60e25f312969>3.2 - ElasticSearch Options</h1>
<div class=lead><a href=https://www.elastic.co>https://www.elastic.co</a></div>
<h3 id=elasticsearch-options>ElasticSearch Options</h3>
<pre tabindex=0><code>--es-archive.bulk.actions int               The number of requests that can be enqueued before the bulk processor decides to commit (default 1000)
--es-archive.bulk.flush-interval duration   A time.Duration after which bulk requests are committed, regardless of other thresholds. Set to zero to disable. By default, this is disabled. (default 200ms)
--es-archive.bulk.size int                  The number of bytes that the bulk requests can take up before the bulk processor decides to commit (default 5000000)
--es-archive.bulk.workers int               The number of workers that are able to receive bulk requests and eventually commit them to Elasticsearch (default 1)
--es-archive.enabled                        Enable extra storage
--es-archive.log-level string               The Elasticsearch client log-level. Valid levels: [debug, info, error] (default &quot;error&quot;)
--es-archive.max-doc-count int              The maximum document count to return from an Elasticsearch query. This will also apply to aggregations. (default 10000)
--es-archive.password string                The password required by Elasticsearch
--es-archive.remote-read-clusters string    Comma-separated list of Elasticsearch remote cluster names for cross-cluster querying.See Elasticsearch remote clusters and cross-cluster query api.
--es-archive.server-urls string             The comma-separated list of Elasticsearch servers, must be full url i.e. http://localhost:9200 (default &quot;http://127.0.0.1:9200&quot;)
--es-archive.sniffer                        The sniffer config for Elasticsearch; client uses sniffing process to find all nodes automatically, disable if not required
--es-archive.sniffer-tls-enabled            Option to enable TLS when sniffing an Elasticsearch Cluster ; client uses sniffing process to find all nodes automatically, disabled by default
--es-archive.timeout duration               Timeout used for queries. A Timeout of zero means no timeout (default 0s)
--es-archive.tls.ca string                  Path to a TLS CA (Certification Authority) file used to verify the remote server(s) (by default will use the system truststore)
--es-archive.tls.cert string                Path to a TLS Certificate file, used to identify this process to the remote server(s)
--es-archive.tls.enabled                    Enable TLS when talking to the remote server(s)
--es-archive.tls.key string                 Path to a TLS Private Key file, used to identify this process to the remote server(s)
--es-archive.tls.server-name string         Override the TLS server name we expect in the certificate of the remote server(s)
--es-archive.tls.skip-host-verify           (insecure) Skip server's certificate chain and host name verification
--es-archive.token-file string              Path to a file containing bearer token. This flag also loads CA if it is specified.
--es-archive.username string                The username required by Elasticsearch. The basic authentication also loads CA if it is specified.
--es-archive.version uint                   The major Elasticsearch version. If not specified, the value will be auto-detected from Elasticsearch.
--es.bulk.actions int                       The number of requests that can be enqueued before the bulk processor decides to commit (default 1000)
--es.bulk.flush-interval duration           A time.Duration after which bulk requests are committed, regardless of other thresholds. Set to zero to disable. By default, this is disabled. (default 200ms)
--es.bulk.size int                          The number of bytes that the bulk requests can take up before the bulk processor decides to commit (default 5000000)
--es.bulk.workers int                       The number of workers that are able to receive bulk requests and eventually commit them to Elasticsearch (default 1)
--es.log-level string                       The Elasticsearch client log-level. Valid levels: [debug, info, error] (default &quot;error&quot;)
--es.max-doc-count int                      The maximum document count to return from an Elasticsearch query. This will also apply to aggregations. (default 10000)
--es.password string                        The password required by Elasticsearch
--es.remote-read-clusters string            Comma-separated list of Elasticsearch remote cluster names for cross-cluster querying.See Elasticsearch remote clusters and cross-cluster query api.
--es.server-urls string                     The comma-separated list of Elasticsearch servers, must be full url i.e. http://localhost:9200 (default &quot;http://127.0.0.1:9200&quot;)
--es.sniffer                                The sniffer config for Elasticsearch; client uses sniffing process to find all nodes automatically, disable if not required
--es.sniffer-tls-enabled                    Option to enable TLS when sniffing an Elasticsearch Cluster ; client uses sniffing process to find all nodes automatically, disabled by default
--es.timeout duration                       Timeout used for queries. A Timeout of zero means no timeout (default 0s)
--es.tls.ca string                          Path to a TLS CA (Certification Authority) file used to verify the remote server(s) (by default will use the system truststore)
--es.tls.cert string                        Path to a TLS Certificate file, used to identify this process to the remote server(s)
--es.tls.enabled                            Enable TLS when talking to the remote server(s)
--es.tls.key string                         Path to a TLS Private Key file, used to identify this process to the remote server(s)
--es.tls.server-name string                 Override the TLS server name we expect in the certificate of the remote server(s)
--es.tls.skip-host-verify                   (insecure) Skip server's certificate chain and host name verification
--es.token-file string                      Path to a file containing bearer token. This flag also loads CA if it is specified.
--es.username string                        The username required by Elasticsearch. The basic authentication also loads CA if it is specified.
--es.version uint                           The major Elasticsearch version. If not specified, the value will be auto-detected from Elasticsearch.
</code></pre>
</div>
<div class=td-content style=page-break-before:always>
<h1 id=pg-92ee0a90a27f32e663d81f846f230895>3.3 - InfluxDB Options</h1>
<div class=lead><a href=https://www.influxdata.com>https://www.influxdata.com</a></div>
<h3 id=influxdb-options>InfluxDB Options</h3>
<pre tabindex=0><code>--influxdb.bucket string                        influx bucket, A bucket is a named location where time series data is stored
--influxdb.http.http-request-timeout duration   HTTP request timeout in sec (default 10s)
--influxdb.org string                           influx organization, An organization is a workspace for a group of users.
--influxdb.tls.ca string                        Path to a TLS CA (Certification Authority) file used to verify the remote server(s) (by default will use the system truststore)
--influxdb.tls.cert string                      Path to a TLS Certificate file, used to identify this process to the remote server(s)
--influxdb.tls.enabled                          Enable TLS when talking to the remote server(s)
--influxdb.tls.key string                       Path to a TLS Private Key file, used to identify this process to the remote server(s)
--influxdb.tls.server-name string               Override the TLS server name we expect in the certificate of the remote server(s)
--influxdb.tls.skip-host-verify                 (insecure) Skip server's certificate chain and host name verification
--influxdb.token string                         Use the Authorization header and the Token scheme
--influxdb.url string                           the influxdb url (default &quot;http://localhost:8086&quot;)
--influxdb.write.batch-size uint                Maximum number of points sent to server in single request (default 5000)
--influxdb.write.default-tags string            Tags added to each point during writing. separated by , (TAG1=VALUE1,TAG2=VALUE2)
--influxdb.write.exponential-base uint          The base for the exponential retry delay (default 2)
--influxdb.write.flush-interval uint            Interval, in ms, in which is buffer flushed if it has not been already written (by reaching batch size) (default 1000)
--influxdb.write.max-retries uint               Maximum count of retry attempts of failed writes (default 5)
--influxdb.write.max-retry-interval uint        The maximum delay between each retry attempt in milliseconds (default 125000)
--influxdb.write.max-retry-time uint            The maximum total retry timeout in millisecond (default 180000)
--influxdb.write.precision duration             Precision to use in writes for timestamp. In unit of duration: time.Nanosecond, time.Microsecond, time.Millisecond, time.Second (default 1ns)
--influxdb.write.retry-buffer-limit uint        Maximum number of points to keep for retry. Should be multiple of BatchSize (default 50000)
--influxdb.write.retry-interval uint            Default retry interval in ms, if not sent by server (default 5000)
--influxdb.write.use-gzip                       Whether to use GZip compression in requests

</code></pre>
</div>
<div class=td-content style=page-break-before:always>
<h1 id=pg-48a7486c70598ab850d05e9edd8674c7>3.4 - Kafka Options</h1>
<div class=lead><a href=https://kafka.apache.org/>https://kafka.apache.org/</a></div>
<h3 id=kafka-options>Kafka Options</h3>
<pre tabindex=0><code>--kafka.producer.authentication string          Authentication type used to authenticate with kafka cluster. e.g. none, kerberos, tls, plaintext (default &quot;none&quot;)
--kafka.producer.batch-linger duration          (experimental) Time interval to wait before sending records to Kafka. Higher value reduce request to Kafka but increase latency and the possibility of data loss in case of process restart. See https://kafka.apache.org/documentation/ (default 0s)
--kafka.producer.batch-max-messages int         (experimental) Maximum number of message to batch before sending records to Kafka
--kafka.producer.batch-min-messages int         (experimental) The best-effort minimum number of messages needed to send a batch of records to Kafka. Higher value reduce request to Kafka but increase latency and the possibility of data loss in case of process restart. See https://kafka.apache.org/documentation/
--kafka.producer.batch-size int                 (experimental) Number of bytes to batch before sending records to Kafka. Higher value reduce request to Kafka but increase latency and the possibility of data loss in case of process restart. See https://kafka.apache.org/documentation/
--kafka.producer.brokers string                 The comma-separated list of kafka brokers. i.e. '127.0.0.1:9092,0.0.0:1234' (default &quot;127.0.0.1:9092&quot;)
--kafka.producer.compression string             (experimental) Type of compression (none, gzip, snappy, lz4, zstd) to use on messages (default &quot;none&quot;)
--kafka.producer.compression-level int          (experimental) compression level to use on messages. gzip = 1-9 (default = 6), snappy = none, lz4 = 1-17 (default = 9), zstd = -131072 - 22 (default = 3)
--kafka.producer.encoding string                Encoding of metric (&quot;json&quot; or &quot;protobuf&quot;) sent to kafka. (default &quot;protobuf&quot;)
--kafka.producer.kerberos.config-file string    Path to Kerberos configuration. i.e /etc/krb5.conf (default &quot;/etc/krb5.conf&quot;)
--kafka.producer.kerberos.keytab-file string    Path to keytab file. i.e /etc/security/kafka.keytab (default &quot;/etc/security/kafka.keytab&quot;)
--kafka.producer.kerberos.password string       The Kerberos password used for authenticate with KDC
--kafka.producer.kerberos.realm string          Kerberos realm
--kafka.producer.kerberos.service-name string   Kerberos service name (default &quot;kafka&quot;)
--kafka.producer.kerberos.use-keytab            Use of keytab instead of password, if this is true, keytab file will be used instead of password
--kafka.producer.kerberos.username string       The Kerberos username used for authenticate with KDC
--kafka.producer.plaintext.mechanism string     The plaintext Mechanism for SASL/PLAIN authentication, e.g. 'SCRAM-SHA-256' or 'SCRAM-SHA-512' or 'PLAIN' (default &quot;PLAIN&quot;)
--kafka.producer.plaintext.password string      The plaintext Password for SASL/PLAIN authentication
--kafka.producer.plaintext.username string      The plaintext Username for SASL/PLAIN authentication
--kafka.producer.protocol-version string        Kafka protocol version - must be supported by kafka server
--kafka.producer.required-acks string           (experimental) Required kafka broker acknowledgement. i.e. noack, local, all (default &quot;local&quot;)
--kafka.producer.tls.ca string                  Path to a TLS CA (Certification Authority) file used to verify the remote server(s) (by default will use the system truststore)
--kafka.producer.tls.cert string                Path to a TLS Certificate file, used to identify this process to the remote server(s)
--kafka.producer.tls.enabled                    Enable TLS when talking to the remote server(s)
--kafka.producer.tls.key string                 Path to a TLS Private Key file, used to identify this process to the remote server(s)
--kafka.producer.tls.server-name string         Override the TLS server name we expect in the certificate of the remote server(s)
--kafka.producer.tls.skip-host-verify           (insecure) Skip server's certificate chain and host name verification
--kafka.producer.topic string                   The name of the kafka topic (default &quot;clymene&quot;)

</code></pre>
</div>
<div class=td-content style=page-break-before:always>
<h1 id=pg-97cfc54549393296293902c0c1b65fdf>3.5 - OpenTSDB Options</h1>
<div class=lead><a href=http://opentsdb.net/>http://opentsdb.net/</a></div>
<h3 id=opentsdb-options>OpenTSDB Options</h3>
<pre tabindex=0><code>--opentsdb.dry-run                       Don't actually send anything to the TSD, just print the datapoints.
--opentsdb.host string                   Hostname to use to connect to the TSD. (default &quot;localhost&quot;)
--opentsdb.hosts string                  List of host:port to connect to tsd's (comma separated)
--opentsdb.http                          Send the data via the http interface (default 'false')
--opentsdb.http-api-path string          URL path to use for HTTP requests to TSD. (default &quot;api/put&quot;)
--opentsdb.http-password string          Password to use for HTTP Basic Auth when sending the data via HTTP
--opentsdb.http-username string          Username to use for HTTP Basic Auth when sending the data via HTTP
--opentsdb.max-chunk int                 The maximum request body size to support for incoming HTTP requests when chunking is enabled (default 512)
--opentsdb.max-tags int                  The maximum number of tags to send to our TSD Instances (default 8)
--opentsdb.port int                      Port to connect to the TSD instance on (default 4242)
--opentsdb.ssl                           Enable SSL - used in conjunction with http (default 'false')
--opentsdb.timeout duration              Time out when doing http insert(sec, default 10 sec) (default 10s)
</code></pre>
</div>
<div class=td-content style=page-break-before:always>
<h1 id=pg-1db9969384a5a8315b7f84b832c15b52>3.6 - Prometheus Options</h1>
<div class=lead><a href=https://prometheus.io/>https://prometheus.io/</a></div>
<h3 id=prometheus-options>Prometheus Options</h3>
<pre tabindex=0><code>--admin.http.host-ports string              The host:ports (e.g. 127.0.0.1:15690 or :15690) for the admin server, including health check, /metrics, etc. (default &quot;:15690&quot;)
--gateway.grpc-server.host-port string      The host:port (e.g. 127.0.0.1:15610 or :15610) of the gateway's GRPC server (default &quot;:15610&quot;)
--gateway.grpc.tls.cert string              Path to a TLS Certificate file, used to identify this server to clients
--gateway.grpc.tls.client-ca string         Path to a TLS CA (Certification Authority) file used to verify certificates presented by clients (if unset, all clients are permitted)
--gateway.grpc.tls.enabled                  Enable TLS on the server
--gateway.grpc.tls.key string               Path to a TLS Private Key file, used to identify this server to clients
--log-level string                          Minimal allowed log Level. For more levels see https://github.com/uber-go/zap (default &quot;info&quot;)
--prometheus.remote.kafka.encoding string   Encoding of metric (&quot;json&quot; or &quot;protobuf&quot;) sent to kafka. (default &quot;protobuf&quot;)
--prometheus.remote.max.err.msg.len int     Maximum length of error message (default 256)
--prometheus.remote.timeout duration        Time out when doing remote write(sec, default 10 sec) (default 10s)
--prometheus.remote.url string              the prometheus remote write receiver endpoint(/api/v1/write) (default &quot;http://localhost:9090/api/v1/write&quot;)
--prometheus.remote.user.agent string       User-Agent in request header (default &quot;Prometheus/&quot;)
</code></pre>
</div>
<div class=td-content style=page-break-before:always>
<h1 id=pg-8dbd4e302cd86a4d0f71f71ce3b300de>4 - Tutorials</h1>
<div class=lead>Introducing a tutorial that allows you to use Clymene in k8s and Docker-compose.</div>
</div>
<div class=td-content>
<h1 id=pg-9f2233bbae28849bca70eebc1abaab70>4.1 - Docker-compose Example</h1>
<div class=lead>How to use Clymene at Docker Compose.</div>
</div>
<div class=td-content style=page-break-before:always>
<h1 id=pg-2193a6415e395bfca415a334c159ad21>4.2 - k8s(Kubernetes) Example</h1>
<div class=lead>How to use Clymene at k8s(kubernetes)</div>
</div>
<div class=td-content style=page-break-before:always>
<h1 id=pg-8efbf7499fe9c7bde8d2c1bb7ff6ee3c>5 - </h1>
</div>
</main>
</div>
</div>
<footer class="bg-dark py-5 row d-print-none">
<div class="container-fluid mx-sm-5">
<div class=row>
<div class="col-6 col-sm-4 text-xs-center order-sm-2">
</div>
<div class="col-6 col-sm-4 text-right text-xs-center order-sm-3">
<ul class="list-inline mb-0">
<li class="list-inline-item mx-2 h3" data-toggle=tooltip data-placement=top title=Twitter aria-label=Twitter>
<a class=text-white target=_blank rel=noopener href=https://twitter.com/clymeneallen aria-label=Twitter>
<i class="fab fa-twitter"></i>
</a>
</li>
<li class="list-inline-item mx-2 h3" data-toggle=tooltip data-placement=top title=GitHub aria-label=GitHub>
<a class=text-white target=_blank rel=noopener href=https://github.com/Clymene-project/Clymene aria-label=GitHub>
<i class="fab fa-github"></i>
</a>
</li>
</ul>
</div>
<div class="col-12 col-sm-4 text-center py-2 order-sm-2">
<small class=text-white>&copy; 2022 The Clymene-project Authors All Rights Reserved</small>
</div>
</div>
</div>
</footer>
</div>
<script src=https://cdn.jsdelivr.net/npm/popper.js@1.16.1/dist/umd/popper.min.js integrity=sha384-9/reFTGAW83EW2RDu2S0VKaIzap3H66lZH81PoYlFhbGU+6BZp6G7niu735Sk7lN crossorigin=anonymous></script>
<script src=https://cdn.jsdelivr.net/npm/bootstrap@4.6.1/dist/js/bootstrap.min.js integrity="sha512-UR25UO94eTnCVwjbXozyeVd6ZqpaAE9naiEUBK/A+QDbfSTQFhPGj5lOR6d8tsgbBk84Ggb5A3EkjsOgPRPcKA==" crossorigin=anonymous></script>
<script src=/js/tabpane-persist.js></script>
<script src=/js/main.min.7380160122c24d59789a168af05fcec445fe4e5f2069dd9f3a0c991f10269ae0.js integrity="sha256-c4AWASLCTVl4mhaK8F/OxEX+Tl8gad2fOgyZHxAmmuA=" crossorigin=anonymous></script>
</body>
</html>