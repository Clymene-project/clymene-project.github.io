<!doctype html><html lang=en class=no-js>
<head>
<meta charset=utf-8>
<meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no">
<meta name=generator content="Hugo 0.92.0">
<link rel=canonical type=text/html href=/docs/database-options/>
<link rel=alternate type=application/rss+xml href=/docs/database-options/index.xml>
<meta name=robots content="noindex, nofollow">
<link rel="shortcut icon" href=/favicons/favicon.ico>
<link rel=apple-touch-icon href=/favicons/apple-touch-icon-180x180.png sizes=180x180>
<link rel=icon type=image/png href=/favicons/favicon-16x16.png sizes=16x16>
<link rel=icon type=image/png href=/favicons/favicon-32x32.png sizes=32x32>
<link rel=icon type=image/png href=/favicons/android-36x36.png sizes=36x36>
<link rel=icon type=image/png href=/favicons/android-48x48.png sizes=48x48>
<link rel=icon type=image/png href=/favicons/android-72x72.png sizes=72x72>
<link rel=icon type=image/png href=/favicons/android-96x96.png sizes=96x96>
<link rel=icon type=image/png href=/favicons/android-144x144.png sizes=144x144>
<link rel=icon type=image/png href=/favicons/android-192x192.png sizes=192x192>
<title>Database Options | Clymene-project</title>
<meta name=description content="Introduce options for the database available.
">
<meta property="og:title" content="Database Options">
<meta property="og:description" content="Introduce options for the database available.
">
<meta property="og:type" content="website">
<meta property="og:url" content="/docs/database-options/"><meta property="og:site_name" content="Clymene-project">
<meta itemprop=name content="Database Options">
<meta itemprop=description content="Introduce options for the database available.
"><meta name=twitter:card content="summary">
<meta name=twitter:title content="Database Options">
<meta name=twitter:description content="Introduce options for the database available.
">
<link rel=preload href=/scss/main.min.704093cd1a56f37fd00dc35c7125fced09d88d60c92d59965b30c8c0eeef1cd5.css as=style>
<link href=/scss/main.min.704093cd1a56f37fd00dc35c7125fced09d88d60c92d59965b30c8c0eeef1cd5.css rel=stylesheet integrity>
<script src=https://code.jquery.com/jquery-3.5.1.min.js integrity="sha256-9/aliU8dGd2tb6OSsuzixeV4y/faTqgFtohetphbbj0=" crossorigin=anonymous></script>
<script type=application/javascript>var doNotTrack=!1;doNotTrack||(window.ga=window.ga||function(){(ga.q=ga.q||[]).push(arguments)},ga.l=+new Date,ga('create','UA-00000000-0','auto'),ga('send','pageview'))</script>
<script async src=https://www.google-analytics.com/analytics.js></script>
</head>
<body class=td-section>
<header>
<nav class="js-navbar-scroll navbar navbar-expand navbar-dark flex-column flex-md-row td-navbar">
<a class=navbar-brand href=/>
<span class=navbar-logo></span><span class="text-uppercase font-weight-bold">Clymene-project</span>
</a>
<div class="td-navbar-nav-scroll ml-md-auto" id=main_navbar>
<ul class="navbar-nav mt-2 mt-lg-0">
<li class="nav-item mr-4 mb-2 mb-lg-0">
<a class="nav-link active" href=/docs/><span class=active>Documentation</span></a>
</li>
<li class="nav-item mr-4 mb-2 mb-lg-0">
<a class=nav-link href=/blog/><span>Releases</span></a>
</li>
</ul>
</div>
<div class="navbar-nav d-none d-lg-block"></div>
</nav>
</header>
<div class="container-fluid td-outer">
<div class=td-main>
<div class="row flex-xl-nowrap">
<main class="col-12 col-md-9 col-xl-8 pl-md-5" role=main>
<div class=td-content>
<div class="pageinfo pageinfo-primary d-print-none">
<p>
This is the multi-page printable view of this section.
<a href=# onclick="return print(),!1">Click here to print</a>.
</p><p>
<a href=/docs/database-options/>Return to the regular view of this page</a>.
</p>
</div>
<h1 class=title>Database Options</h1>
<div class=lead>Introduce options for the database available.</div>
<ul>
<li>1: <a href=#pg-637ffb17e80bb3ade4e7d9b697cfee37>Cortex Options</a></li>
<li>2: <a href=#pg-842fa5a3c1a3dc527d1c60e25f312969>ElasticSearch Options</a></li>
<li>3: <a href=#pg-92ee0a90a27f32e663d81f846f230895>InfluxDB Options</a></li>
<li>4: <a href=#pg-48a7486c70598ab850d05e9edd8674c7>Kafka Options</a></li>
<li>5: <a href=#pg-97cfc54549393296293902c0c1b65fdf>OpenTSDB Options</a></li>
<li>6: <a href=#pg-1db9969384a5a8315b7f84b832c15b52>Prometheus Options</a></li>
</ul>
<div class=content>
<p>Describe the database options available in Clymene</p>
</div>
</div>
<div class=td-content>
<h1 id=pg-637ffb17e80bb3ade4e7d9b697cfee37>1 - Cortex Options</h1>
<div class=lead><a href=https://cortexmetrics.io/>https://cortexmetrics.io/</a></div>
<h3 id=cortex-options>Cortex Options</h3>
<pre tabindex=0><code>--cortex.distributor.kafka.encoding string   Encoding of metric (&quot;json&quot; or &quot;protobuf&quot;) sent to kafka. (default &quot;protobuf&quot;)
--cortex.distributor.max.err.msg.len int     Maximum length of error message (default 256)
--cortex.distributor.timeout duration        Time out when doing remote write(sec, default 10 sec) (default 10s)
--cortex.distributor.url string              the cortex distributor remote write receiver endpoint(/api/v1/push) (default &quot;http://localhost/api/v1/push&quot;)
--cortex.distributor.user.agent string       User-Agent in request header (default &quot;Prometheus/&quot;)
</code></pre>
</div>
<div class=td-content style=page-break-before:always>
<h1 id=pg-842fa5a3c1a3dc527d1c60e25f312969>2 - ElasticSearch Options</h1>
<div class=lead><a href=https://www.elastic.co>https://www.elastic.co</a></div>
<h3 id=elasticsearch-options>ElasticSearch Options</h3>
<pre tabindex=0><code>--es-archive.bulk.actions int               The number of requests that can be enqueued before the bulk processor decides to commit (default 1000)
--es-archive.bulk.flush-interval duration   A time.Duration after which bulk requests are committed, regardless of other thresholds. Set to zero to disable. By default, this is disabled. (default 200ms)
--es-archive.bulk.size int                  The number of bytes that the bulk requests can take up before the bulk processor decides to commit (default 5000000)
--es-archive.bulk.workers int               The number of workers that are able to receive bulk requests and eventually commit them to Elasticsearch (default 1)
--es-archive.enabled                        Enable extra storage
--es-archive.log-level string               The Elasticsearch client log-level. Valid levels: [debug, info, error] (default &quot;error&quot;)
--es-archive.max-doc-count int              The maximum document count to return from an Elasticsearch query. This will also apply to aggregations. (default 10000)
--es-archive.password string                The password required by Elasticsearch
--es-archive.remote-read-clusters string    Comma-separated list of Elasticsearch remote cluster names for cross-cluster querying.See Elasticsearch remote clusters and cross-cluster query api.
--es-archive.server-urls string             The comma-separated list of Elasticsearch servers, must be full url i.e. http://localhost:9200 (default &quot;http://127.0.0.1:9200&quot;)
--es-archive.sniffer                        The sniffer config for Elasticsearch; client uses sniffing process to find all nodes automatically, disable if not required
--es-archive.sniffer-tls-enabled            Option to enable TLS when sniffing an Elasticsearch Cluster ; client uses sniffing process to find all nodes automatically, disabled by default
--es-archive.timeout duration               Timeout used for queries. A Timeout of zero means no timeout (default 0s)
--es-archive.tls.ca string                  Path to a TLS CA (Certification Authority) file used to verify the remote server(s) (by default will use the system truststore)
--es-archive.tls.cert string                Path to a TLS Certificate file, used to identify this process to the remote server(s)
--es-archive.tls.enabled                    Enable TLS when talking to the remote server(s)
--es-archive.tls.key string                 Path to a TLS Private Key file, used to identify this process to the remote server(s)
--es-archive.tls.server-name string         Override the TLS server name we expect in the certificate of the remote server(s)
--es-archive.tls.skip-host-verify           (insecure) Skip server's certificate chain and host name verification
--es-archive.token-file string              Path to a file containing bearer token. This flag also loads CA if it is specified.
--es-archive.username string                The username required by Elasticsearch. The basic authentication also loads CA if it is specified.
--es-archive.version uint                   The major Elasticsearch version. If not specified, the value will be auto-detected from Elasticsearch.
--es.bulk.actions int                       The number of requests that can be enqueued before the bulk processor decides to commit (default 1000)
--es.bulk.flush-interval duration           A time.Duration after which bulk requests are committed, regardless of other thresholds. Set to zero to disable. By default, this is disabled. (default 200ms)
--es.bulk.size int                          The number of bytes that the bulk requests can take up before the bulk processor decides to commit (default 5000000)
--es.bulk.workers int                       The number of workers that are able to receive bulk requests and eventually commit them to Elasticsearch (default 1)
--es.log-level string                       The Elasticsearch client log-level. Valid levels: [debug, info, error] (default &quot;error&quot;)
--es.max-doc-count int                      The maximum document count to return from an Elasticsearch query. This will also apply to aggregations. (default 10000)
--es.password string                        The password required by Elasticsearch
--es.remote-read-clusters string            Comma-separated list of Elasticsearch remote cluster names for cross-cluster querying.See Elasticsearch remote clusters and cross-cluster query api.
--es.server-urls string                     The comma-separated list of Elasticsearch servers, must be full url i.e. http://localhost:9200 (default &quot;http://127.0.0.1:9200&quot;)
--es.sniffer                                The sniffer config for Elasticsearch; client uses sniffing process to find all nodes automatically, disable if not required
--es.sniffer-tls-enabled                    Option to enable TLS when sniffing an Elasticsearch Cluster ; client uses sniffing process to find all nodes automatically, disabled by default
--es.timeout duration                       Timeout used for queries. A Timeout of zero means no timeout (default 0s)
--es.tls.ca string                          Path to a TLS CA (Certification Authority) file used to verify the remote server(s) (by default will use the system truststore)
--es.tls.cert string                        Path to a TLS Certificate file, used to identify this process to the remote server(s)
--es.tls.enabled                            Enable TLS when talking to the remote server(s)
--es.tls.key string                         Path to a TLS Private Key file, used to identify this process to the remote server(s)
--es.tls.server-name string                 Override the TLS server name we expect in the certificate of the remote server(s)
--es.tls.skip-host-verify                   (insecure) Skip server's certificate chain and host name verification
--es.token-file string                      Path to a file containing bearer token. This flag also loads CA if it is specified.
--es.username string                        The username required by Elasticsearch. The basic authentication also loads CA if it is specified.
--es.version uint                           The major Elasticsearch version. If not specified, the value will be auto-detected from Elasticsearch.
</code></pre>
</div>
<div class=td-content style=page-break-before:always>
<h1 id=pg-92ee0a90a27f32e663d81f846f230895>3 - InfluxDB Options</h1>
<div class=lead><a href=https://www.influxdata.com>https://www.influxdata.com</a></div>
<h3 id=influxdb-options>InfluxDB Options</h3>
<pre tabindex=0><code>--influxdb.bucket string                        influx bucket, A bucket is a named location where time series data is stored
--influxdb.http.http-request-timeout duration   HTTP request timeout in sec (default 10s)
--influxdb.org string                           influx organization, An organization is a workspace for a group of users.
--influxdb.tls.ca string                        Path to a TLS CA (Certification Authority) file used to verify the remote server(s) (by default will use the system truststore)
--influxdb.tls.cert string                      Path to a TLS Certificate file, used to identify this process to the remote server(s)
--influxdb.tls.enabled                          Enable TLS when talking to the remote server(s)
--influxdb.tls.key string                       Path to a TLS Private Key file, used to identify this process to the remote server(s)
--influxdb.tls.server-name string               Override the TLS server name we expect in the certificate of the remote server(s)
--influxdb.tls.skip-host-verify                 (insecure) Skip server's certificate chain and host name verification
--influxdb.token string                         Use the Authorization header and the Token scheme
--influxdb.url string                           the influxdb url (default &quot;http://localhost:8086&quot;)
--influxdb.write.batch-size uint                Maximum number of points sent to server in single request (default 5000)
--influxdb.write.default-tags string            Tags added to each point during writing. separated by , (TAG1=VALUE1,TAG2=VALUE2)
--influxdb.write.exponential-base uint          The base for the exponential retry delay (default 2)
--influxdb.write.flush-interval uint            Interval, in ms, in which is buffer flushed if it has not been already written (by reaching batch size) (default 1000)
--influxdb.write.max-retries uint               Maximum count of retry attempts of failed writes (default 5)
--influxdb.write.max-retry-interval uint        The maximum delay between each retry attempt in milliseconds (default 125000)
--influxdb.write.max-retry-time uint            The maximum total retry timeout in millisecond (default 180000)
--influxdb.write.precision duration             Precision to use in writes for timestamp. In unit of duration: time.Nanosecond, time.Microsecond, time.Millisecond, time.Second (default 1ns)
--influxdb.write.retry-buffer-limit uint        Maximum number of points to keep for retry. Should be multiple of BatchSize (default 50000)
--influxdb.write.retry-interval uint            Default retry interval in ms, if not sent by server (default 5000)
--influxdb.write.use-gzip                       Whether to use GZip compression in requests

</code></pre>
</div>
<div class=td-content style=page-break-before:always>
<h1 id=pg-48a7486c70598ab850d05e9edd8674c7>4 - Kafka Options</h1>
<div class=lead><a href=https://kafka.apache.org/>https://kafka.apache.org/</a></div>
<h3 id=kafka-options>Kafka Options</h3>
<pre tabindex=0><code>--kafka.producer.authentication string          Authentication type used to authenticate with kafka cluster. e.g. none, kerberos, tls, plaintext (default &quot;none&quot;)
--kafka.producer.batch-linger duration          (experimental) Time interval to wait before sending records to Kafka. Higher value reduce request to Kafka but increase latency and the possibility of data loss in case of process restart. See https://kafka.apache.org/documentation/ (default 0s)
--kafka.producer.batch-max-messages int         (experimental) Maximum number of message to batch before sending records to Kafka
--kafka.producer.batch-min-messages int         (experimental) The best-effort minimum number of messages needed to send a batch of records to Kafka. Higher value reduce request to Kafka but increase latency and the possibility of data loss in case of process restart. See https://kafka.apache.org/documentation/
--kafka.producer.batch-size int                 (experimental) Number of bytes to batch before sending records to Kafka. Higher value reduce request to Kafka but increase latency and the possibility of data loss in case of process restart. See https://kafka.apache.org/documentation/
--kafka.producer.brokers string                 The comma-separated list of kafka brokers. i.e. '127.0.0.1:9092,0.0.0:1234' (default &quot;127.0.0.1:9092&quot;)
--kafka.producer.compression string             (experimental) Type of compression (none, gzip, snappy, lz4, zstd) to use on messages (default &quot;none&quot;)
--kafka.producer.compression-level int          (experimental) compression level to use on messages. gzip = 1-9 (default = 6), snappy = none, lz4 = 1-17 (default = 9), zstd = -131072 - 22 (default = 3)
--kafka.producer.encoding string                Encoding of metric (&quot;json&quot; or &quot;protobuf&quot;) sent to kafka. (default &quot;protobuf&quot;)
--kafka.producer.kerberos.config-file string    Path to Kerberos configuration. i.e /etc/krb5.conf (default &quot;/etc/krb5.conf&quot;)
--kafka.producer.kerberos.keytab-file string    Path to keytab file. i.e /etc/security/kafka.keytab (default &quot;/etc/security/kafka.keytab&quot;)
--kafka.producer.kerberos.password string       The Kerberos password used for authenticate with KDC
--kafka.producer.kerberos.realm string          Kerberos realm
--kafka.producer.kerberos.service-name string   Kerberos service name (default &quot;kafka&quot;)
--kafka.producer.kerberos.use-keytab            Use of keytab instead of password, if this is true, keytab file will be used instead of password
--kafka.producer.kerberos.username string       The Kerberos username used for authenticate with KDC
--kafka.producer.plaintext.mechanism string     The plaintext Mechanism for SASL/PLAIN authentication, e.g. 'SCRAM-SHA-256' or 'SCRAM-SHA-512' or 'PLAIN' (default &quot;PLAIN&quot;)
--kafka.producer.plaintext.password string      The plaintext Password for SASL/PLAIN authentication
--kafka.producer.plaintext.username string      The plaintext Username for SASL/PLAIN authentication
--kafka.producer.protocol-version string        Kafka protocol version - must be supported by kafka server
--kafka.producer.required-acks string           (experimental) Required kafka broker acknowledgement. i.e. noack, local, all (default &quot;local&quot;)
--kafka.producer.tls.ca string                  Path to a TLS CA (Certification Authority) file used to verify the remote server(s) (by default will use the system truststore)
--kafka.producer.tls.cert string                Path to a TLS Certificate file, used to identify this process to the remote server(s)
--kafka.producer.tls.enabled                    Enable TLS when talking to the remote server(s)
--kafka.producer.tls.key string                 Path to a TLS Private Key file, used to identify this process to the remote server(s)
--kafka.producer.tls.server-name string         Override the TLS server name we expect in the certificate of the remote server(s)
--kafka.producer.tls.skip-host-verify           (insecure) Skip server's certificate chain and host name verification
--kafka.producer.topic string                   The name of the kafka topic (default &quot;clymene&quot;)

</code></pre>
</div>
<div class=td-content style=page-break-before:always>
<h1 id=pg-97cfc54549393296293902c0c1b65fdf>5 - OpenTSDB Options</h1>
<div class=lead><a href=http://opentsdb.net/>http://opentsdb.net/</a></div>
<h3 id=opentsdb-options>OpenTSDB Options</h3>
<pre tabindex=0><code>--opentsdb.dry-run                       Don't actually send anything to the TSD, just print the datapoints.
--opentsdb.host string                   Hostname to use to connect to the TSD. (default &quot;localhost&quot;)
--opentsdb.hosts string                  List of host:port to connect to tsd's (comma separated)
--opentsdb.http                          Send the data via the http interface (default 'false')
--opentsdb.http-api-path string          URL path to use for HTTP requests to TSD. (default &quot;api/put&quot;)
--opentsdb.http-password string          Password to use for HTTP Basic Auth when sending the data via HTTP
--opentsdb.http-username string          Username to use for HTTP Basic Auth when sending the data via HTTP
--opentsdb.max-chunk int                 The maximum request body size to support for incoming HTTP requests when chunking is enabled (default 512)
--opentsdb.max-tags int                  The maximum number of tags to send to our TSD Instances (default 8)
--opentsdb.port int                      Port to connect to the TSD instance on (default 4242)
--opentsdb.ssl                           Enable SSL - used in conjunction with http (default 'false')
--opentsdb.timeout duration              Time out when doing http insert(sec, default 10 sec) (default 10s)
</code></pre>
</div>
<div class=td-content style=page-break-before:always>
<h1 id=pg-1db9969384a5a8315b7f84b832c15b52>6 - Prometheus Options</h1>
<div class=lead><a href=https://prometheus.io/>https://prometheus.io/</a></div>
<h3 id=prometheus-options>Prometheus Options</h3>
<pre tabindex=0><code>--admin.http.host-ports string              The host:ports (e.g. 127.0.0.1:15690 or :15690) for the admin server, including health check, /metrics, etc. (default &quot;:15690&quot;)
--gateway.grpc-server.host-port string      The host:port (e.g. 127.0.0.1:15610 or :15610) of the gateway's GRPC server (default &quot;:15610&quot;)
--gateway.grpc.tls.cert string              Path to a TLS Certificate file, used to identify this server to clients
--gateway.grpc.tls.client-ca string         Path to a TLS CA (Certification Authority) file used to verify certificates presented by clients (if unset, all clients are permitted)
--gateway.grpc.tls.enabled                  Enable TLS on the server
--gateway.grpc.tls.key string               Path to a TLS Private Key file, used to identify this server to clients
--log-level string                          Minimal allowed log Level. For more levels see https://github.com/uber-go/zap (default &quot;info&quot;)
--prometheus.remote.kafka.encoding string   Encoding of metric (&quot;json&quot; or &quot;protobuf&quot;) sent to kafka. (default &quot;protobuf&quot;)
--prometheus.remote.max.err.msg.len int     Maximum length of error message (default 256)
--prometheus.remote.timeout duration        Time out when doing remote write(sec, default 10 sec) (default 10s)
--prometheus.remote.url string              the prometheus remote write receiver endpoint(/api/v1/write) (default &quot;http://localhost:9090/api/v1/write&quot;)
--prometheus.remote.user.agent string       User-Agent in request header (default &quot;Prometheus/&quot;)
</code></pre>
</div>
</main>
</div>
</div>
<footer class="bg-dark py-5 row d-print-none">
<div class="container-fluid mx-sm-5">
<div class=row>
<div class="col-6 col-sm-4 text-xs-center order-sm-2">
</div>
<div class="col-6 col-sm-4 text-right text-xs-center order-sm-3">
<ul class="list-inline mb-0">
<li class="list-inline-item mx-2 h3" data-toggle=tooltip data-placement=top title=Twitter aria-label=Twitter>
<a class=text-white target=_blank rel=noopener href=https://twitter.com/clymeneallen aria-label=Twitter>
<i class="fab fa-twitter"></i>
</a>
</li>
<li class="list-inline-item mx-2 h3" data-toggle=tooltip data-placement=top title=GitHub aria-label=GitHub>
<a class=text-white target=_blank rel=noopener href=https://github.com/Clymene-project/Clymene aria-label=GitHub>
<i class="fab fa-github"></i>
</a>
</li>
</ul>
</div>
<div class="col-12 col-sm-4 text-center py-2 order-sm-2">
<small class=text-white>&copy; 2022 The Clymene-project Authors All Rights Reserved</small>
</div>
</div>
</div>
</footer>
</div>
<script src=https://cdn.jsdelivr.net/npm/popper.js@1.16.1/dist/umd/popper.min.js integrity=sha384-9/reFTGAW83EW2RDu2S0VKaIzap3H66lZH81PoYlFhbGU+6BZp6G7niu735Sk7lN crossorigin=anonymous></script>
<script src=https://cdn.jsdelivr.net/npm/bootstrap@4.6.1/dist/js/bootstrap.min.js integrity="sha512-UR25UO94eTnCVwjbXozyeVd6ZqpaAE9naiEUBK/A+QDbfSTQFhPGj5lOR6d8tsgbBk84Ggb5A3EkjsOgPRPcKA==" crossorigin=anonymous></script>
<script src=/js/tabpane-persist.js></script>
<script src=/js/main.min.7380160122c24d59789a168af05fcec445fe4e5f2069dd9f3a0c991f10269ae0.js integrity="sha256-c4AWASLCTVl4mhaK8F/OxEX+Tl8gad2fOgyZHxAmmuA=" crossorigin=anonymous></script>
</body>
</html>